{
 "cells": [
  {
   "cell_type": "code",
   "id": "c72116e7",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import torch\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "from scipy.sparse import issparse\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "import argparse\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.trainer import (\n",
    "    prepare_data,\n",
    "    test\n",
    ")\n",
    "from scgpt.model.model_prompt import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.preprocess import Preprocessor, TFPreprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
    "from scgpt.reproduction_util import load_and_process_data, get_weighted_sampler"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17d2d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_parameters = dict(\n",
    "    dataset_name=\"ms\",  # Dataset name  （ms/COVID/NSCLC/MergedMonkey/mouse_115746/mouse_10x/mouse_smart/elegans）\n",
    "    model_path=\"../checkpoint/celltype_identification\",  # Path to peft model\n",
    "    data_path=\"../data/celltype_identification\",# Path to dataset\n",
    "    peft_type=\"Encoder_adapter\"  # Encoder_adapter/ Token_adapter / Prefix / LoRA / finetune\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ebe5d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_defaults = dict(\n",
    "    seed=0,\n",
    "    dataset_name=key_parameters[\"dataset_name\"],\n",
    "    load_model=key_parameters[\"model_path\"]+f\"/{key_parameters['dataset_name']}/{key_parameters['peft_type']}\",\n",
    "    mask_ratio=0.0,\n",
    "    n_bins=51,\n",
    "    MVC=False, # Masked value prediction for cell embedding\n",
    "    ecs_thres=0.0, # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=0.0,\n",
    "    lr=1e-4,\n",
    "    batch_size=20,\n",
    "    layer_size=128,\n",
    "    nlayers=4,  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead=4,  # number of heads in nn.MultiheadAttention\n",
    "    dropout=0.2,  # dropout probability\n",
    "    schedule_ratio=0.9,  # ratio of epochs for learning rate schedule\n",
    "    save_eval_interval=5,\n",
    "    fast_transformer= False,\n",
    "    pre_norm=False,\n",
    "    amp=True,  # Automatic Mixed Precision\n",
    "    include_zero_gene = False,\n",
    "    freeze = False, #freeze\n",
    "    DSBN = False,  # Domain-spec batchnorm\n",
    "    data_path=key_parameters[\"data_path\"],\n",
    "    prompt_type=key_parameters[\"peft_type\"],  \n",
    "    num_tokens=64,\n",
    "    n_layers_conf=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],  # token\n",
    "    mlp_adapter_conf=[1, 1, 1, 1, 1, 1, 0,0,0,0,0,0],\n",
    "    space_adapter_conf=[1, 1, 1, 1, 1, 1,0,0,0,0,0,0],\n",
    "    input_style=\"binned\",\n",
    "    max_seq_len=2001,\n",
    "    pad_token = \"<pad>\",\n",
    "    pad_value=-2,\n",
    "    input_layer_key=\"X_binned\",\n",
    "    mask_value=-1,\n",
    "    use_batch_labels=False\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e9a8078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(seed=0, dataset_name='ms', load_model='../checkpoint/celltype_identification/ms/Encoder_adapter', mask_ratio=0.0, n_bins=51, MVC=False, ecs_thres=0.0, dab_weight=0.0, lr=0.0001, batch_size=20, layer_size=128, nlayers=4, nhead=4, dropout=0.2, schedule_ratio=0.9, save_eval_interval=5, fast_transformer=False, pre_norm=False, amp=True, include_zero_gene=False, freeze=False, DSBN=False, data_path='../data/celltype_identification', prompt_type='encoder-prompt', num_tokens=64, n_layers_conf=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], mlp_adapter_conf=[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], space_adapter_conf=[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], input_style='binned', max_seq_len=2001, pad_token='<pad>', pad_value=-2, input_layer_key='X_binned', mask_value=-1, use_batch_labels=False, model_path='../checkpoint/celltype_identification', peft_type='Encoder_adapter', use_prompt=True)\n"
     ]
    }
   ],
   "source": [
    "peft_prompt_relationship = {\n",
    "    \"Encoder_adapter\": \"encoder-prompt\",\n",
    "    \"Token_adapter\": \"head-prompt\",\n",
    "    \"Prefix\": \"prefix-prompt\",\n",
    "    \"LoRA\": \"LoRA\",\n",
    "    \"finetune\": \"finetune\"\n",
    "}\n",
    "hyperparameter_defaults.update(key_parameters)\n",
    "config = argparse.Namespace(**hyperparameter_defaults)\n",
    "config.prompt_type = peft_prompt_relationship[config.peft_type]\n",
    "config.use_prompt = False if config.prompt_type == \"finetune\" else True\n",
    "print(config)\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eb284d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for input and preprocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_ratio = config.mask_ratio\n",
    "mask_value = \"auto\"  # for masked values, now it should always be auto\n",
    "\n",
    "include_zero_gene = config.include_zero_gene  # if True, include zero genes among hvgs in the training\n",
    "max_seq_len = 2001\n",
    "n_bins = config.n_bins\n",
    "\n",
    "# input/output representation\n",
    "input_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "output_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "\n",
    "# settings for training\n",
    "MLM = False  # whether to use masked language modeling, currently it is always on.\n",
    "CLS = True  # celltype classification objective\n",
    "ADV = False  # Adversarial training for batch correction\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "MVC = config.MVC  # Masked value prediction for cell embedding\n",
    "ECS = config.ecs_thres > 0  # Elastic cell similarity objective\n",
    "DAB = False  # Domain adaptation by reverse backpropagation, set to 2 for separate optimizer\n",
    "INPUT_BATCH_LABELS = False  # TODO: have these help MLM and MVC, while not to classifier\n",
    "input_emb_style = \"continuous\"  # \"category\" or \"continuous\" or \"scaling\"\n",
    "cell_emb_style = \"cls\"  # \"avg-pool\" or \"w-pool\" or \"cls\"\n",
    "adv_E_delay_epochs = 0  # delay adversarial training on encoder for a few epochs\n",
    "adv_D_delay_epochs = 0\n",
    "mvc_decoder_style = \"inner product\"\n",
    "ecs_threshold = config.ecs_thres\n",
    "dab_weight = config.dab_weight\n",
    "\n",
    "explicit_zero_prob = MLM and include_zero_gene  # whether explicit bernoulli for zeros\n",
    "do_sample_in_train = False and explicit_zero_prob  # sample the bernoulli in training\n",
    "\n",
    "per_seq_batch_sample = False\n",
    "\n",
    "# settings for optimizer\n",
    "lr = config.lr  # TODO: test learning rate ratio between two tasks\n",
    "lr_ADV = 1e-3  # learning rate for discriminator, used when ADV is True\n",
    "batch_size = config.batch_size\n",
    "eval_batch_size = config.batch_size\n",
    "schedule_interval = 1\n",
    "\n",
    "# settings for the model\n",
    "fast_transformer = config.fast_transformer\n",
    "fast_transformer_backend = \"flash\"  # \"linear\" or \"flash\"\n",
    "embsize = config.layer_size  # embedding dimension\n",
    "d_hid = config.layer_size  # dimension of the feedforward network in TransformerEncoder\n",
    "nlayers = config.nlayers  # number of TransformerEncoderLayer in TransformerEncoder\n",
    "nhead = config.nhead  # number of heads in nn.MultiheadAttention\n",
    "dropout = config.dropout  # dropout probability\n",
    "\n",
    "data_path = config.data_path\n",
    "use_prompt = config.use_prompt\n",
    "prompt_type = config.prompt_type\n",
    "num_tokens = config.num_tokens\n",
    "n_layers_conf = config.n_layers_conf\n",
    "mlp_adapter_conf = config.mlp_adapter_conf\n",
    "space_adapter_conf = config.space_adapter_conf\n",
    "\n",
    "# logging\n",
    "log_interval = 100  # iterations\n",
    "save_eval_interval = config.save_eval_interval  # epochs\n",
    "do_eval_scib_metrics = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b5b2f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% validate settings\n",
    "assert input_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert output_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert input_emb_style in [\"category\", \"continuous\", \"scaling\"]\n",
    "if input_style == \"binned\":\n",
    "    if input_emb_style == \"scaling\":\n",
    "        raise ValueError(\"input_emb_style `scaling` is not supported for binned input.\")\n",
    "elif input_style == \"log1p\" or input_style == \"normed_raw\":\n",
    "    if input_emb_style == \"category\":\n",
    "        raise ValueError(\n",
    "            \"input_emb_style `category` is not supported for log1p or normed_raw input.\"\n",
    "        )\n",
    "\n",
    "if input_emb_style == \"category\":\n",
    "    mask_value = n_bins + 1\n",
    "    pad_value = n_bins  # for padding gene expr values\n",
    "    n_input_bins = n_bins + 2\n",
    "else:\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = n_bins\n",
    "\n",
    "if ADV and DAB:\n",
    "    raise ValueError(\"ADV and DAB cannot be both True.\")\n",
    "DAB_separate_optim = True if DAB > 1 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1513a424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to save/ms/encoder-prompt\n"
     ]
    }
   ],
   "source": [
    "dataset_name = config.dataset_name\n",
    "save_dir = Path(f\"./save/{dataset_name}/{prompt_type}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"save to {save_dir}\")\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")\n",
    "\n",
    "data_dir = Path(data_path + dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e46629af",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "prompt_settings = {\n",
    "    \"use_prompt\": config.use_prompt,\n",
    "    \"num_tokens\": config.num_tokens,\n",
    "    \"prompt_type\": config.prompt_type,\n",
    "    \"n_layers_conf\": config.n_layers_conf,\n",
    "    \"mlp_adapter_conf\": config.mlp_adapter_conf,\n",
    "    \"space_adapter_conf\": config.space_adapter_conf\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5086b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/anndata/__init__.py:55: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n",
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/anndata/__init__.py:55: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n",
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/anndata/__init__.py:55: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n",
      "/users/PCON0022/coffee19850519/NENU/scGPT/tutorials/../scgpt/reproduction_util.py:52: FutureWarning: Use anndata.concat instead of AnnData.concatenate, AnnData.concatenate is deprecated and will be removed in the future. See the tutorial for concat at: https://anndata.readthedocs.io/en/latest/concatenation.html\n",
      "  adata = adata.concatenate((adata_val, adata_test), batch_key=\"str_batch\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 2000/2000 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:169: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n",
      "scGPT - INFO - train set number of samples: 15751, \n",
      "\t feature length: 762\n",
      "scGPT - INFO - valid set number of samples: 1649, \n",
      "\t feature length: 668\n",
      "scGPT - INFO - Resume model from ../checkpoint/celltype_identification/ms/Encoder_adapter/model_fold0.pt, the model args will override the config ../checkpoint/celltype_identification/ms/Encoder_adapter/args.json.\n",
      "scGPT - INFO - <All keys matched successfully>\n",
      "scGPT - INFO - fold0:Accuracy: 0.880, Precision: 0.822, Recall: 0.880, Macro F1: 0.843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/anndata/__init__.py:55: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n",
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/anndata/__init__.py:55: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n",
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/anndata/__init__.py:55: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 2001/2001 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Normalizing total counts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PCON0022/coffee19850519/NENU/scGPT/tutorials/../scgpt/reproduction_util.py:52: FutureWarning: Use anndata.concat instead of AnnData.concatenate, AnnData.concatenate is deprecated and will be removed in the future. See the tutorial for concat at: https://anndata.readthedocs.io/en/latest/concatenation.html\n",
      "  adata = adata.concatenate((adata_val, adata_test), batch_key=\"str_batch\")\n",
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:169: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Binning data ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n",
      "scGPT - INFO - train set number of samples: 12943, \n",
      "\t feature length: 809\n",
      "scGPT - INFO - valid set number of samples: 1698, \n",
      "\t feature length: 706\n",
      "scGPT - INFO - Resume model from ../checkpoint/celltype_identification/ms/Encoder_adapter/model_fold1.pt, the model args will override the config ../checkpoint/celltype_identification/ms/Encoder_adapter/args.json.\n",
      "scGPT - INFO - <All keys matched successfully>\n",
      "scGPT - INFO - fold1:Accuracy: 0.879, Precision: 0.838, Recall: 0.879, Macro F1: 0.853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/anndata/__init__.py:55: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n",
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/anndata/__init__.py:55: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n",
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/anndata/__init__.py:55: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 2000/2000 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Normalizing total counts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PCON0022/coffee19850519/NENU/scGPT/tutorials/../scgpt/reproduction_util.py:52: FutureWarning: Use anndata.concat instead of AnnData.concatenate, AnnData.concatenate is deprecated and will be removed in the future. See the tutorial for concat at: https://anndata.readthedocs.io/en/latest/concatenation.html\n",
      "  adata = adata.concatenate((adata_val, adata_test), batch_key=\"str_batch\")\n",
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:169: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Binning data ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n",
      "scGPT - INFO - train set number of samples: 17081, \n",
      "\t feature length: 793\n",
      "scGPT - INFO - valid set number of samples: 1194, \n",
      "\t feature length: 694\n",
      "scGPT - INFO - Resume model from ../checkpoint/celltype_identification/ms/Encoder_adapter/model_fold2.pt, the model args will override the config ../checkpoint/celltype_identification/ms/Encoder_adapter/args.json.\n",
      "scGPT - INFO - <All keys matched successfully>\n",
      "scGPT - INFO - fold2:Accuracy: 0.870, Precision: 0.828, Recall: 0.870, Macro F1: 0.846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/anndata/__init__.py:55: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n",
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/anndata/__init__.py:55: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n",
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/anndata/__init__.py:55: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n",
      "/users/PCON0022/coffee19850519/NENU/scGPT/tutorials/../scgpt/reproduction_util.py:52: FutureWarning: Use anndata.concat instead of AnnData.concatenate, AnnData.concatenate is deprecated and will be removed in the future. See the tutorial for concat at: https://anndata.readthedocs.io/en/latest/concatenation.html\n",
      "  adata = adata.concatenate((adata_val, adata_test), batch_key=\"str_batch\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 2000/2000 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:169: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n",
      "scGPT - INFO - train set number of samples: 16214, \n",
      "\t feature length: 815\n",
      "scGPT - INFO - valid set number of samples: 1279, \n",
      "\t feature length: 701\n",
      "scGPT - INFO - Resume model from ../checkpoint/celltype_identification/ms/Encoder_adapter/model_fold3.pt, the model args will override the config ../checkpoint/celltype_identification/ms/Encoder_adapter/args.json.\n",
      "scGPT - INFO - <All keys matched successfully>\n",
      "scGPT - INFO - fold3:Accuracy: 0.913, Precision: 0.844, Recall: 0.862, Macro F1: 0.851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/anndata/__init__.py:55: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n",
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/anndata/__init__.py:55: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n",
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/anndata/__init__.py:55: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n",
      "/users/PCON0022/coffee19850519/NENU/scGPT/tutorials/../scgpt/reproduction_util.py:52: FutureWarning: Use anndata.concat instead of AnnData.concatenate, AnnData.concatenate is deprecated and will be removed in the future. See the tutorial for concat at: https://anndata.readthedocs.io/en/latest/concatenation.html\n",
      "  adata = adata.concatenate((adata_val, adata_test), batch_key=\"str_batch\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 2001/2001 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:169: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n",
      "scGPT - INFO - train set number of samples: 15580, \n",
      "\t feature length: 781\n",
      "scGPT - INFO - valid set number of samples: 1279, \n",
      "\t feature length: 658\n",
      "scGPT - INFO - Resume model from ../checkpoint/celltype_identification/ms/Encoder_adapter/model_fold4.pt, the model args will override the config ../checkpoint/celltype_identification/ms/Encoder_adapter/args.json.\n",
      "scGPT - INFO - <All keys matched successfully>\n",
      "scGPT - INFO - fold4:Accuracy: 0.900, Precision: 0.854, Recall: 0.900, Macro F1: 0.866\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_splits):\n",
    "\n",
    "    if config.load_model is not None:\n",
    "        model_dir = Path(config.load_model)\n",
    "        model_config_file = model_dir / \"args.json\"\n",
    "        model_file = model_dir / f\"model_fold{i}.pt\"\n",
    "        vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "        vocab = GeneVocab.from_file(vocab_file)\n",
    "        # shutil.copy(vocab_file, save_dir / \"vocab.json\")\n",
    "        for s in special_tokens:\n",
    "            if s not in vocab:\n",
    "                vocab.append_token(s)\n",
    "        tokenized_data, data_global_describe = load_and_process_data(dataset_name, i, config, vocab, logger)\n",
    "        # model\n",
    "        with open(model_config_file, \"r\") as f:\n",
    "            model_configs = json.load(f)\n",
    "        logger.info(\n",
    "            f\"Resume model from {model_file}, the model args will override the \"\n",
    "            f\"config {model_config_file}.\"\n",
    "        )\n",
    "        embsize = model_configs[\"embsize\"]\n",
    "        nhead = model_configs[\"nheads\"]\n",
    "        d_hid = model_configs[\"d_hid\"]\n",
    "        nlayers = model_configs[\"nlayers\"]\n",
    "        n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    n_cls = torch.load(model_file,map_location=device)['cls_decoder.out_layer.bias'].shape[0]\n",
    "    ntokens = len(vocab)  # size of vocabulary\n",
    "    model = TransformerModel(\n",
    "        ntokens,\n",
    "        embsize,\n",
    "        nhead,\n",
    "        d_hid,\n",
    "        nlayers,\n",
    "        nlayers_cls=3,\n",
    "        n_cls=n_cls,\n",
    "        vocab=vocab,\n",
    "        dropout=config.dropout,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        do_mvc=MVC,\n",
    "        do_dab=DAB,\n",
    "        use_batch_labels=INPUT_BATCH_LABELS,\n",
    "        domain_spec_batchnorm=config.DSBN,\n",
    "        input_emb_style=input_emb_style,\n",
    "        n_input_bins=n_input_bins,\n",
    "        cell_emb_style=cell_emb_style,\n",
    "        mvc_decoder_style=mvc_decoder_style,\n",
    "        ecs_threshold=ecs_threshold,\n",
    "        explicit_zero_prob=explicit_zero_prob,\n",
    "        use_fast_transformer=fast_transformer,\n",
    "        fast_transformer_backend=fast_transformer_backend,\n",
    "        pre_norm=config.pre_norm,\n",
    "        **prompt_settings\n",
    "    )\n",
    "\n",
    "    if config.prompt_type == \"LoRA\":\n",
    "        model_weights = torch.load(model_file, map_location=device)\n",
    "        if 'transformer_encoder.layers.0.self_attn.Wqkv.weight' in model_weights:\n",
    "            for i in range(6):\n",
    "                del model_weights[f'transformer_encoder.layers.{i}.self_attn.in_proj_weight']\n",
    "                del model_weights[f'transformer_encoder.layers.{i}.self_attn.in_proj_bias']\n",
    "            model.load_state_dict(model_weights, strict=True)\n",
    "            logger.info(\"<All keys matched successfully>\")\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(model_file,map_location=device), strict=True)\n",
    "        logger.info(\"<All keys matched successfully>\")\n",
    "    model.to(device)\n",
    "    adata_test = data_global_describe[\"adata_test\"]\n",
    "    predictions, labels, results = test(\n",
    "        model=model,\n",
    "        adata=adata_test,\n",
    "        gene_ids=data_global_describe[\"gene_ids\"],\n",
    "        vocab=vocab,\n",
    "        config=config,\n",
    "        device=device,\n",
    "        logger=logger\n",
    "    )\n",
    "    logger.info(f\"fold{i}:\"\n",
    "        f\"Accuracy: {results['test/accuracy']:.3f}, Precision: {results['test/precision']:.3f}, Recall: {results['test/recall']:.3f}, \"\n",
    "        f\"Macro F1: {results['test/macro_f1']:.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e236ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scGPT",
   "language": "python",
   "name": "scgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
