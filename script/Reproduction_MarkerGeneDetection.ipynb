{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc6f53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/users/PCON0022/coffee19850519/NENU/scGPT/tutorials/../scgpt/model/model.py:21: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/users/PCON0022/coffee19850519/NENU/scGPT/tutorials/../scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/users/PCON0022/coffee19850519/NENU/scGPT/tutorials/../scgpt/model/model_prompt.py:21: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/scanpy/_settings.py:450: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  IPython.display.set_matplotlib_formats(*ipython_format)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import  pandas as pd\n",
    "from einops import rearrange\n",
    "from gears import PertData, GEARS\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import numpy as np\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "import argparse\n",
    "import scib\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "from scgpt.trainer import SeqDataset\n",
    "import scgpt as scg\n",
    "from scgpt.model.model_prompt import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.preprocess import Preprocessor, TFPreprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, load_pretrained\n",
    "\n",
    "sc.set_figure_params(figsize=(4, 4))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b329d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_parameters = dict(\n",
    "    dataset_name=\"COVID\",  # Dataset name\n",
    "    model_path=\"../checkpoint/marker_gene_detection\",  # Path to peft model\n",
    "    data_path=\"../data/\",# Path to dataset\n",
    "    peft_type=\"Encoder_adapter\"  # Encoder_adapter/ Token_adapter / Prefix / LoRA / finetune\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b299dcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_defaults = dict(\n",
    "    seed=0,\n",
    "    dataset_name=key_parameters[\"dataset_name\"],\n",
    "    load_model=key_parameters[\"model_path\"]+f\"/{key_parameters['dataset_name']}/{key_parameters['peft_type']}\",\n",
    "    mask_ratio=0.0,\n",
    "    n_bins=51,\n",
    "    MVC=False, # Masked value prediction for cell embedding\n",
    "    ecs_thres=0.0, # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=0.0,\n",
    "    lr=1e-4,\n",
    "    layer_size=128,\n",
    "    batch_size=1,\n",
    "    nlayers=4,  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead=4,  # number of heads in nn.MultiheadAttention\n",
    "    dropout=0.2,  # dropout probability\n",
    "    schedule_ratio=0.9,  # ratio of epochs for learning rate schedule\n",
    "    save_eval_interval=5,\n",
    "    fast_transformer= False,  \n",
    "    pre_norm=False,\n",
    "    amp=True,  # Automatic Mixed Precision\n",
    "    include_zero_gene = True,\n",
    "    freeze = False, #freeze\n",
    "    DSBN = False,  # Domain-spec batchnorm\n",
    "    data_path=key_parameters[\"data_path\"],\n",
    "    prompt_type=key_parameters[\"peft_type\"],  # prefix_prompt/Gene_encoder_prompt/Gene_token_prompt/LoRA\n",
    "    num_tokens=64,\n",
    "    n_layers_conf=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],  # token\n",
    "    mlp_adapter_conf=[0, 0, 0,0, 0, 0, 0,0,0,0,0,0],\n",
    "    space_adapter_conf=[1, 1, 1, 1, 1, 1,0,0,0,0,0,0],\n",
    "    input_style=\"binned\",\n",
    "    max_seq_len=2001,\n",
    "    pad_token = \"<pad>\",\n",
    "    pad_value=-2,\n",
    "    input_layer_key=\"X_binned\",\n",
    "    mask_value=-1,\n",
    "    use_batch_labels=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "190855e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(seed=0, dataset_name='COVID', load_model='../checkpoint/marker_gene_detection/COVID/Encoder_adapter', mask_ratio=0.0, n_bins=51, MVC=False, ecs_thres=0.0, dab_weight=0.0, lr=0.0001, layer_size=128, batch_size=1, nlayers=4, nhead=4, dropout=0.2, schedule_ratio=0.9, save_eval_interval=5, fast_transformer=False, pre_norm=False, amp=True, include_zero_gene=True, freeze=False, DSBN=False, data_path='../data/', prompt_type='encoder-prompt', num_tokens=64, n_layers_conf=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], mlp_adapter_conf=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], space_adapter_conf=[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], input_style='binned', max_seq_len=2001, pad_token='<pad>', pad_value=-2, input_layer_key='X_binned', mask_value=-1, use_batch_labels=False, model_path='../checkpoint/marker_gene_detection', peft_type='Encoder_adapter', use_prompt=True)\n"
     ]
    }
   ],
   "source": [
    "peft_prompt_relationship = {\n",
    "    \"Encoder_adapter\": \"encoder-prompt\",\n",
    "    \"Token_adapter\": \"head-prompt\",\n",
    "    \"Prefix\": \"prefix-prompt\",\n",
    "    \"LoRA\": \"LoRA\",\n",
    "    \"finetune\": \"finetune\"\n",
    "}\n",
    "hyperparameter_defaults.update(key_parameters)\n",
    "config = argparse.Namespace(**hyperparameter_defaults)\n",
    "config.prompt_type = peft_prompt_relationship[config.peft_type]\n",
    "config.use_prompt = False if config.prompt_type == \"finetune\" else True\n",
    "print(config)\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f55665aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for input and preprocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "# mask_ratio = config.mask_ratio\n",
    "mask_value = -1\n",
    "pad_value = -2\n",
    "n_input_bins = config.n_bins\n",
    "\n",
    "n_hvg = 2000  # number of highly variable genes\n",
    "max_seq_len = n_hvg + 1\n",
    "per_seq_batch_sample = False\n",
    "DSBN = False  # Domain-spec batchnorm\n",
    "explicit_zero_prob = config.include_zero_gene  # whether explicit bernoulli for zeros\n",
    "include_zero_gene = config.include_zero_gene\n",
    "\n",
    "dataset_name = config.dataset_name\n",
    "logger = scg.logger\n",
    "data_dir = config.data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1321709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == 'COVID':\n",
    "    adata = sc.read(f\"{data_dir}/{dataset_name}_test0.h5ad\")\n",
    "    n_hvg = False\n",
    "    data_is_raw = True\n",
    "    filter_gene_by_counts = False\n",
    "    adata.obs[\"celltype\"] = adata.obs[\"cell_type\"].astype(\"category\")\n",
    "    adata.var[\"gene_name\"] = adata.var.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ae74a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 1843/2000 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from ../checkpoint/marker_gene_detection/COVID/Encoder_adapter/model_fold0.pt, the model args will be overriden by the config ../checkpoint/marker_gene_detection/COVID-19/Encoder_adapter/args.json.\n"
     ]
    }
   ],
   "source": [
    "if config.load_model is not None:\n",
    "    model_dir = Path(config.load_model)\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"model_fold0.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will be overriden by the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d06a951b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Log1p transforming ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=filter_gene_by_counts,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=config.n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "preprocessor(adata, batch_key=None)\n",
    "input_layer_key = \"X_binned\"\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb5e1ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - <All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (encoder): GeneEncoder(\n",
       "    (embedding): Embedding(60697, 512, padding_idx=60694)\n",
       "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (value_encoder): ContinuousValueEncoder(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
       "    (activation): ReLU()\n",
       "    (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (prompt_dropout): Dropout(p=0, inplace=False)\n",
       "        (prompt_proj): Identity()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        (Space_Adapter): Adapter(\n",
       "          (act): GELU(approximate='none')\n",
       "          (D_fc1): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (D_fc2): Linear(in_features=128, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (6-11): 6 x TransformerEncoderLayer(\n",
       "        (prompt_dropout): Dropout(p=0, inplace=False)\n",
       "        (prompt_proj): Identity()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ExprDecoder(\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (cls_decoder): ClsDecoder(\n",
       "    (_decoder): ModuleList(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (out_layer): Linear(in_features=512, out_features=16, bias=True)\n",
       "  )\n",
       "  (sim): Similarity(\n",
       "    (cos): CosineSimilarity()\n",
       "  )\n",
       "  (creterion_cce): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "prompt_settings = {\n",
    "    \"use_prompt\": config.use_prompt,\n",
    "    \"num_tokens\": config.num_tokens,\n",
    "    \"prompt_type\": config.prompt_type,\n",
    "    \"n_layers_conf\": config.n_layers_conf,\n",
    "    \"mlp_adapter_conf\": config.mlp_adapter_conf,\n",
    "    \"space_adapter_conf\": config.space_adapter_conf\n",
    "}\n",
    "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_cls = torch.load(model_file, map_location=device)['cls_decoder.out_layer.bias'].shape[0]\n",
    "\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=3,\n",
    "    n_cls=n_cls,\n",
    "    vocab=vocab,\n",
    "    dropout=config.dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=config.MVC,\n",
    "    do_dab=False,\n",
    "    use_batch_labels=False,\n",
    "    domain_spec_batchnorm=config.DSBN,\n",
    "    n_input_bins=n_input_bins,\n",
    "    cell_emb_style=\"cls\",\n",
    "    mvc_decoder_style=\"inner product\",\n",
    "    ecs_threshold=config.ecs_thres,\n",
    "    explicit_zero_prob=False,\n",
    "    use_fast_transformer=config.fast_transformer,\n",
    "    fast_transformer_backend=\"flash\",\n",
    "    pre_norm=config.pre_norm,\n",
    "    **prompt_settings\n",
    ")\n",
    "if config.load_model is not None:\n",
    "    model.load_state_dict(torch.load(model_file, map_location=device), strict=True)\n",
    "    logger.info(\"<All keys matched successfully>\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8ea807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attention(\n",
    "        model: nn.Module,\n",
    "        batch_size: int = 1,\n",
    "        num_attn_layers: int = 11,\n",
    "        return_new_adata: bool = True,\n",
    ") -> Optional[AnnData]:\n",
    "    \"\"\"extract_attention on dataset of adata_t\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=True):\n",
    "        M = all_gene_ids.size(1)\n",
    "        N = all_gene_ids.size(0)\n",
    "\n",
    "        count = 0\n",
    "        cell_gene_correlations = np.zeros((N, len(gene_ids)), dtype=np.float32)\n",
    "\n",
    "        for i in tqdm(range(0, N, batch_size), desc=\"Embedding attention scores\"):\n",
    "            # Replicate the operations in model forward pass\n",
    "            src_embs = model.encoder(torch.tensor(all_gene_ids[i: i + batch_size], dtype=torch.long).to(device))\n",
    "            val_embs = model.value_encoder(torch.tensor(all_values[i: i + batch_size], dtype=torch.float).to(device))\n",
    "            total_embs = src_embs + val_embs\n",
    "            # Send total_embs to attention layers for attention operations\n",
    "            # Retrieve the output from second to last layer\n",
    "            for layer in model.transformer_encoder.layers[:num_attn_layers]:\n",
    "                total_embs = layer(total_embs, src_key_padding_mask=src_key_padding_mask[i: i + batch_size].to(device))\n",
    "\n",
    "            # Send total_embs to the last layer in native-attn\n",
    "            attn_weight = model.transformer_encoder.layers[num_attn_layers].self_attn.in_proj_weight\n",
    "            attn_bias = model.transformer_encoder.layers[num_attn_layers].self_attn.in_proj_bias\n",
    "            qkv = F.linear(total_embs, attn_weight, attn_bias)\n",
    "\n",
    "            # qkv = model.transformer_encoder.layers[num_attn_layers].self_attn.Wqkv(total_embs)\n",
    "            # Retrieve q, k, and v from native-attn wrapper\n",
    "            qkv = rearrange(qkv, 'b s (three h d) -> b s three h d', three=3, h=8)\n",
    "            q = qkv[:, :, 0, :, :]\n",
    "            k = qkv[:, :, 1, :, :]\n",
    "            v = qkv[:, :, 2, :, :]\n",
    "\n",
    "            attn_scores = q.permute(0, 2, 1, 3) @ k.permute(0, 2, 3, 1)\n",
    "\n",
    "            # Rank normalization by row\n",
    "            attn_scores = attn_scores.reshape((-1, M))\n",
    "            order = torch.argsort(attn_scores, dim=1)\n",
    "            rank = torch.argsort(order, dim=1)\n",
    "            attn_scores = rank.reshape((-1, 8, M, M)) / M\n",
    "\n",
    "            # Rank normalization by column\n",
    "            attn_scores = attn_scores.permute(0, 1, 3, 2).reshape((-1, M))\n",
    "            order = torch.argsort(attn_scores, dim=1)\n",
    "            rank = torch.argsort(order, dim=1)\n",
    "            attn_scores = (rank.reshape((-1, 8, M, M)) / M).permute(0, 1, 3, 2)\n",
    "            attn_scores = attn_scores.mean(1)\n",
    "\n",
    "            # Except cls token\n",
    "            outputs = attn_scores[:, 0, 1:].detach().cpu().numpy()\n",
    "            cell_gene_correlations[count: count + len(outputs)] = outputs\n",
    "            count += len(outputs)\n",
    "\n",
    "    if return_new_adata:\n",
    "        return sc.AnnData(X=cell_gene_correlations, obs=adata.obs, var=adata.var, dtype=\"float32\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32a0ceb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - data set number of samples: 21394, feature length: 1844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding attention scores: 100%|██████████| 21394/21394 [09:54<00:00, 35.97it/s]\n"
     ]
    }
   ],
   "source": [
    "input_layer_key = \"X_binned\"\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "\n",
    "tokenized_all = tokenize_and_pad_batch(\n",
    "    all_counts,\n",
    "    gene_ids,\n",
    "    max_len=len(genes) + 1,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f\"data set number of samples: {tokenized_all['genes'].shape[0]}, \"\n",
    "    f\"feature length: {tokenized_all['genes'].shape[1]}\"\n",
    ")\n",
    "all_gene_ids, all_values = tokenized_all[\"genes\"], tokenized_all[\"values\"]\n",
    "src_key_padding_mask = all_gene_ids.eq(vocab[pad_token])\n",
    "\n",
    "# Use this argument to specify which layer to extract the attention weights from\n",
    "# Default to 11, extraction from the last (12th) layer. Note that index starts from 0\n",
    "num_attn_layers = 11\n",
    "\n",
    "attn_adata = extract_attention(\n",
    "    model=model,\n",
    "    batch_size=config.batch_size,\n",
    "    num_attn_layers=num_attn_layers,\n",
    "    return_new_adata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e31813ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify top k number of genes to be selected, and the selection setting\n",
    "topk = 20\n",
    "setting = 'difference'\n",
    "assert setting in [\"difference\", \"target\"]\n",
    "\n",
    "# Only used in 'difference'\n",
    "target_celltype = [\"Effector Memory CD8+ T\"]\n",
    "background_celltype = [\"Memory CD8+ T\"]\n",
    "setting=\"difference\"\n",
    "if setting == 'difference':\n",
    "    case = np.mean(attn_adata[attn_adata.obs[\"celltype\"].isin(target_celltype)].X, axis=0)\n",
    "    control = np.mean(attn_adata[attn_adata.obs[\"celltype\"].isin(background_celltype)].X, axis=0)\n",
    "\n",
    "    enrichment_scores = case - control\n",
    "    gene_symbol = attn_adata.var.index.tolist()\n",
    "# Sort in descending order based on enrichment score\n",
    "df_gene_enrichment = pd.DataFrame({'gene_symbol': gene_symbol, 'scores': enrichment_scores})\n",
    "df_gene_enrichment = df_gene_enrichment.sort_values(by='scores', ascending=False)\n",
    "\n",
    "df_gene_enrichment.to_csv(f\"enrichment_scores_{config.prompt_type}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059b683e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scGPT",
   "language": "python",
   "name": "scgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
