{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a2939a",
   "metadata": {},
   "source": [
    "# Reproduce PEFT(Parameter-Efficient Fine-Tuning) on Pre-trained Model with Perturbation\n",
    "In this tutorial, we demonstrate how to reproduce a PEFT (Parameter-Efficient Fine-Tuning) pre-trained model on a specific dataset for the perturbation task. This tutorial serves as a practical example. There are two steps that need to be executed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e9cf3e",
   "metadata": {},
   "source": [
    "## Step 1: Modify the parameters\n",
    "\n",
    "### There are four key settings that the reader needs to modify. The available options are listed below:\n",
    "***dataset_name*** : adamson / norman   \n",
    "***load_model***: {checkpoint_path}/perturbation  \n",
    "***data_path***: {data_path}/perturbation    \n",
    "***peft_type***: Encoder_adapter/ Token_adapter / Prefix / LoRA / finetune.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e602e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_parameters = dict(\n",
    "    dataset_name=\"adamson\",  # Dataset name\n",
    "    load_model=\"../save/perturbation\",  # Path to peft model\n",
    "    data_path=\"../data/perturbation\",\n",
    "    peft_type=\"Encoder_adapter\",  # Encoder_adapter/ Token_adapter / Prefix / LoRA / finetune\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53e5fccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PCON0022/coffee19850519/ondemand/scPEFT_reproduction/publication/../scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/users/PCON0022/coffee19850519/.conda/envs/scGPT/lib/python3.10/site-packages/wandb/env.py:16: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  from distutils.util import strtobool\n",
      "/users/PCON0022/coffee19850519/ondemand/scPEFT_reproduction/publication/../scgpt/model/model_prompt.py:21: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from torch_geometric.loader import DataLoader\n",
    "from gears import PertData, GEARS\n",
    "from gears.inference import compute_metrics, deeper_analysis, non_dropout_analysis\n",
    "import argparse\n",
    "import traceback\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.model.generation_model_prompt import TransformerGenerator\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.utils import set_seed, map_raw_id_to_vocab_id, load_pretrained\n",
    "\n",
    "matplotlib.rcParams[\"savefig.transparent\"] = False\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69fa419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = argparse.Namespace(**key_parameters)\n",
    "peft_prompt_relationship = {\n",
    "    \"Encoder_adapter\": \"encoder-prompt\",\n",
    "    \"Token_adapter\": \"head-prompt\",\n",
    "    \"Prefix\": \"prefix-prompt\",\n",
    "    \"LoRA\": \"LoRA\",\n",
    "    \"finetune\": \"finetune\"\n",
    "}\n",
    "\n",
    "config.prompt_type = peft_prompt_relationship[config.peft_type]\n",
    "config.use_prompt = False if config.prompt_type == \"finetune\" else True\n",
    "\n",
    "# settings for data prcocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "pad_value = 0  # for padding values\n",
    "pert_pad_id = 2\n",
    "\n",
    "n_hvg = 0  # number of highly variable genes\n",
    "include_zero_gene = \"all\"  # include zero expr genes in training input, \"all\", \"batch-wise\", \"row-wise\", or False\n",
    "max_seq_len = 1536\n",
    "\n",
    "# settings for training\n",
    "MLM = True  # whether to use masked language modeling, currently it is always on.\n",
    "CLS = False  # celltype classification objective\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "MVC = False  # Masked value prediction for cell embedding\n",
    "ECS = False  # Elastic cell similarity objective\n",
    "cell_emb_style = \"cls\"\n",
    "mvc_decoder_style = \"inner product, detach\"\n",
    "amp = True\n",
    "\n",
    "# settings for optimizer\n",
    "batch_size = 20\n",
    "eval_batch_size = 20\n",
    "\n",
    "# settings for the model\n",
    "embsize = 512  # embedding dimension\n",
    "d_hid = 512  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 12  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8  # number of heads in nn.MultiheadAttention\n",
    "n_layers_cls = 3\n",
    "dropout = 0.2  # dropout probability\n",
    "use_fast_transformer = False  # whether to use fast transformer\n",
    "\n",
    "# settings for prompt model\n",
    "use_prompt = config.use_prompt\n",
    "prompt_type = config.prompt_type\n",
    "num_tokens = 20\n",
    "\n",
    "n_layers_conf = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # token\n",
    "mlp_adapter_conf = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "space_adapter_conf = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cf29ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of split is detected. Loading...\n",
      "Simulation split test composition:\n",
      "combo_seen0:0\n",
      "combo_seen1:0\n",
      "combo_seen2:0\n",
      "unseen_single:22\n",
      "Done!\n",
      "Creating dataloaders....\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# dataset and evaluation choices\n",
    "split = \"simulation\"\n",
    "data_name = config.dataset_name\n",
    "\n",
    "\n",
    "pert_data = PertData(config.data_path)\n",
    "pert_data.load(data_name=data_name)\n",
    "pert_data.prepare_split(split=split, seed=1)\n",
    "pert_data.get_dataloader(batch_size=batch_size, test_batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e864c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.load_model is not None:\n",
    "    model_dir = Path(config.load_model, config.dataset_name, config.peft_type)\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    pert_data.adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in pert_data.adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(pert_data.adata.var[\"id_in_vocab\"])\n",
    "    genes = pert_data.adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "else:\n",
    "    genes = pert_data.adata.var[\"gene_name\"].tolist()\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(\n",
    "    [vocab[gene] if gene in vocab else vocab[\"<pad>\"] for gene in genes], dtype=int\n",
    ")\n",
    "n_genes = len(genes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62295071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using simple batchnorm instead of domain specific batchnorm\n",
      "Loading all model params from ../save/perturbation/adamson/Encoder_adapter/best_model.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerGenerator(\n",
       "  (encoder): GeneEncoder(\n",
       "    (embedding): Embedding(60697, 512, padding_idx=60694)\n",
       "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (value_encoder): ContinuousValueEncoder(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
       "    (activation): ReLU()\n",
       "    (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pert_encoder): Embedding(3, 512, padding_idx=2)\n",
       "  (bn): BatchNorm1d(512, eps=6.1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (prompt_dropout): Dropout(p=0, inplace=False)\n",
       "        (prompt_proj): Identity()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        (Space_Adapter): Adapter(\n",
       "          (act): GELU(approximate='none')\n",
       "          (D_fc1): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (D_fc2): Linear(in_features=128, out_features=512, bias=True)\n",
       "        )\n",
       "        (MLP_Adapter): Adapter(\n",
       "          (act): GELU(approximate='none')\n",
       "          (D_fc1): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (D_fc2): Linear(in_features=128, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (6-11): 6 x TransformerEncoderLayer(\n",
       "        (prompt_dropout): Dropout(p=0, inplace=False)\n",
       "        (prompt_proj): Identity()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ExprDecoder(\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (sim): Similarity(\n",
       "    (cos): CosineSimilarity()\n",
       "  )\n",
       "  (creterion_cce): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TransformerGenerator(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=n_layers_cls,\n",
    "    n_cls=1,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    pert_pad_id=pert_pad_id,\n",
    "    do_mvc=MVC,\n",
    "    cell_emb_style=cell_emb_style,\n",
    "    mvc_decoder_style=mvc_decoder_style,\n",
    "    use_fast_transformer=use_fast_transformer,\n",
    "    # Prompt settings\n",
    "    use_prompt=use_prompt,\n",
    "    num_tokens=num_tokens,\n",
    "    prompt_type=prompt_type,\n",
    "    n_layers_conf=n_layers_conf,\n",
    "    mlp_adapter_conf=mlp_adapter_conf,\n",
    "    space_adapter_conf=space_adapter_conf,\n",
    ")\n",
    "\n",
    "if config.load_model is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file, map_location=device))\n",
    "        print(f\"Loading all model params from {model_file}\")\n",
    "    except Exception as e:\n",
    "        use_flash_attn = getattr(model, \"use_fast_transformer\", True)\n",
    "        pretrained_dict = torch.load(model_file, map_location='cpu')\n",
    "\n",
    "        pretrained_dict = {\n",
    "            k.replace(\"Wqkv.\", \"in_proj_\"): v for k, v in pretrained_dict.items()\n",
    "        }\n",
    "\n",
    "        model_dict = model.state_dict()\n",
    "        not_in_pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in model_dict.items()\n",
    "            if not (k in pretrained_dict and v.shape == model_dict[k].shape)\n",
    "        }\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        print(\"Check the results, if there are some problems please contact the developer\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eeb7300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_perturb(\n",
    "        loader: DataLoader, model: TransformerGenerator, device: torch.device\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run model in inference mode using a given data loader\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    pert_cat = []\n",
    "    pred = []\n",
    "    truth = []\n",
    "    pred_de = []\n",
    "    truth_de = []\n",
    "    results = {}\n",
    "    logvar = []\n",
    "\n",
    "    for itr, batch in enumerate(loader):\n",
    "        batch.to(device)\n",
    "        pert_cat.extend(batch.pert)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            p = model.pred_perturb(batch, include_zero_gene, gene_ids=gene_ids)\n",
    "            t = batch.y\n",
    "            pred.extend(p.cpu())\n",
    "            truth.extend(t.cpu())\n",
    "\n",
    "            # Differentially expressed genes\n",
    "            for itr, de_idx in enumerate(batch.de_idx):\n",
    "                pred_de.append(p[itr, de_idx])\n",
    "                truth_de.append(t[itr, de_idx])\n",
    "\n",
    "    # all genes\n",
    "    results[\"pert_cat\"] = np.array(pert_cat)\n",
    "    pred = torch.stack(pred)\n",
    "    truth = torch.stack(truth)\n",
    "    results[\"pred\"] = pred.detach().cpu().numpy().astype(np.float64)\n",
    "    results[\"truth\"] = truth.detach().cpu().numpy().astype(np.float64)\n",
    "\n",
    "    pred_de = torch.stack(pred_de)\n",
    "    truth_de = torch.stack(truth_de)\n",
    "    results[\"pred_de\"] = pred_de.detach().cpu().numpy().astype(np.float64)\n",
    "    results[\"truth_de\"] = truth_de.detach().cpu().numpy().astype(np.float64)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9706d244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 0.006074607088877525, 'mse_de': 0.134563381264249, 'pearson': 0.9900001383379734, 'pearson_de': 0.9748845078713599}\n"
     ]
    }
   ],
   "source": [
    "test_loader = pert_data.dataloader[\"test_loader\"]\n",
    "test_res = eval_perturb(test_loader, model, device)\n",
    "test_metrics, test_pert_res = compute_metrics(test_res)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743a30b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scGPT",
   "language": "python",
   "name": "scgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
