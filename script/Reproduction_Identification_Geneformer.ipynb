{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "614bd9c1bd48901d",
   "metadata": {},
   "source": [
    "# Reproduce PEFT(Parameter-Efficient Fine-Tuning) on Pre-trained Model(Geneformer) with Identification\n",
    "In this tutorial, we demonstrate how to reproduce a PEFT (Parameter-Efficient Fine-Tuning) pre-trained model(Geneformer) on a specific dataset for the identification cell type task. This tutorial serves as a practical example. There are two steps that need to be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a9dbbaa2386d5b",
   "metadata": {},
   "source": [
    "## Step 1: Modify the parameters\n",
    "### There are four key settings that the reader needs to modify. The available options are listed below:\n",
    "***dataset_name*** : NSCLC/COVID/ms, \\\n",
    "***data_path***: {data_path}, \\\n",
    "***output_path***: {output_path}, \\\n",
    "***model_path***: {checkpoint_path},\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c05cd6969b1cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--dataset_name\", type=str, default='ms',help='NSCLC/COVID/ms')\n",
    "parser.add_argument(\"--data_path\", type=str, default='../data/celltype_identification/', help='Path of data for predicting')\n",
    "parser.add_argument(\"--output_path\", type=str, default=f\"../Geneformer/\", help='output data path.')\n",
    "parser.add_argument(\"--model_path\", type=str, default=f\"../checkpoint/celltype_identification/COVID/Encoder_adapter/\", help='Path of data for predicting.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35da5a223a1d46",
   "metadata": {},
   "source": [
    "## Step 2: Start the validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "151b4a356d54d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import datasets\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "from accelerate.utils import is_datasets_available\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformerslocal.src.transformers.trainer_utils import seed_worker\n",
    "\n",
    "from numpy import mean\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformerslocal.src.transformers import EarlyStoppingCallback\n",
    "\n",
    "# device\n",
    "# device = torch.device(\"cuda:3\")\n",
    "# import\n",
    "import torch.nn as nn\n",
    "# imports\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import pickle\n",
    "import subprocess\n",
    "import seaborn as sns;\n",
    "\n",
    "sns.set()\n",
    "from datasets import load_from_disk\n",
    "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, precision_score, recall_score\n",
    "from transformerslocal.src.transformers.models.bert.modeling_bert import BertForSequenceClassification\n",
    "# from transformers  import BertForSequenceClassification\n",
    "from transformers import Trainer\n",
    "from transformers.training_args import TrainingArguments\n",
    "from geneformer import DataCollatorForCellClassification\n",
    "import numpy as np\n",
    "import loralib as lora\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument(\"--epoch\", type=int, default=100, help='Number of epochs.')\n",
    "parser.add_argument(\"--use_prompt\", type=bool, default=True, help='whether use prompt or not.')\n",
    "parser.add_argument(\"--lr\", type=int, default=1e-4, help='')\n",
    "parser.add_argument(\"--batch_size\", type=int, default=10, help='')\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eb3b74399fa2062",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_name=args.dataset_name\n",
    "model_path = args.model_path\n",
    "data_dir =args.data_path\n",
    "accuracys=[]\n",
    "precisions=[]\n",
    "recalls=[]\n",
    "macro_f1s=[]\n",
    "mic_precisions=[]\n",
    "mic_recalls=[]\n",
    "micro_f1s=[]\n",
    "if dataset_name==\"MergedHuman\":\n",
    "    n_splits = 4\n",
    "else:\n",
    "    n_splits = 5\n",
    "\n",
    "i=1\n",
    "\n",
    "# for i in range(n_splits):\n",
    "\n",
    "train_data_dir = f'{data_dir}/{dataset_name}/{i}/{dataset_name}_train{i}.dataset'\n",
    "eval_data_dir = f'{data_dir}/{dataset_name}/{i}/{dataset_name}_val{i}.dataset'\n",
    "test_data_dir = f'{data_dir}/{dataset_name}/{i}/{dataset_name}_test{i}.dataset'\n",
    "train_dataset = load_from_disk(train_data_dir)\n",
    "eval_dataset = load_from_disk(eval_data_dir)\n",
    "test_dataset = load_from_disk(test_data_dir)\n",
    "\n",
    "dataset_list = []\n",
    "evalset_list = []\n",
    "organ_list = []\n",
    "testset_list = []\n",
    "target_dict_list = []\n",
    "\n",
    "trainset_organ = train_dataset\n",
    "evalset_organ = eval_dataset\n",
    "testset_organ = test_dataset\n",
    "# per scDeepsort published method, drop cell types representing <0.5% of cells\n",
    "celltype_counter = Counter(trainset_organ[\"cell_type\"])\n",
    "total_cells = sum(celltype_counter.values())\n",
    "cells_to_keep = [k for k, v in celltype_counter.items() if v > (0.005 * total_cells)]\n",
    "\n",
    "trainset_organ_subset = trainset_organ\n",
    "evalset_organ_subset = evalset_organ\n",
    "testset_organ_subset = testset_organ\n",
    "\n",
    "# shuffle datasets and rename columns\n",
    "trainset_organ_shuffled = trainset_organ_subset.shuffle(seed=42)\n",
    "trainset_organ_shuffled = trainset_organ_shuffled.rename_column(\"cell_type\", \"label\")\n",
    "\n",
    "evalset_organ_shuffled = evalset_organ_subset.shuffle(seed=42)\n",
    "evalset_organ_shuffled = evalset_organ_shuffled.rename_column(\"cell_type\", \"label\")\n",
    "\n",
    "testset_organ_shuffled = testset_organ_subset.shuffle(seed=42)\n",
    "testset_organ_shuffled = testset_organ_shuffled.rename_column(\"cell_type\", \"label\")\n",
    "\n",
    "# create dictionary of cell types : label ids\n",
    "target_names = list(Counter(trainset_organ_shuffled[\"label\"] + evalset_organ_shuffled[\"label\"] + testset_organ_shuffled[\"label\"]).keys())\n",
    "target_name_id_dict = dict(zip(target_names, [i for i in range(len(target_names))]))\n",
    "target_dict_list += [target_name_id_dict]\n",
    "target_name_id_celltype_dict = {value: key for key, value in target_name_id_dict.items()}\n",
    "\n",
    "# change labels to numerical ids\n",
    "def classes_to_ids(example):\n",
    "    example[\"label\"] = target_name_id_dict[example[\"label\"]]\n",
    "    return example\n",
    "\n",
    "labeled_trainset = trainset_organ_shuffled.map(classes_to_ids, num_proc=16)\n",
    "labeled_evalset = evalset_organ_shuffled.map(classes_to_ids, num_proc=16)\n",
    "labeled_testset = testset_organ_shuffled.map(classes_to_ids, num_proc=16)\n",
    "\n",
    "labeled_train_split = labeled_trainset\n",
    "labeled_eval_split = labeled_evalset\n",
    "\n",
    "labeled_test_split_subset = labeled_testset\n",
    "trained_labels = list(Counter(labeled_train_split[\"label\"]).keys())\n",
    "labeled_eval_split_subset = labeled_eval_split\n",
    "\n",
    "dataset_list += [labeled_train_split]\n",
    "evalset_list += [labeled_eval_split_subset]\n",
    "testset_list += [labeled_test_split_subset]\n",
    "# %%\n",
    "\n",
    "trainset_dict = dataset_list\n",
    "traintargetdict_dict = target_dict_list\n",
    "evalset_dict = evalset_list\n",
    "testset_dict = testset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28f9eacf6fae3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SaveProcessResult(labels, pred_cellcodes, confidence_socre):\n",
    "    d_save = {'pred_celltype': [target_name_id_celltype_dict[pred_cellcode] for pred_cellcode in pred_cellcodes],\n",
    "              'gt_celltype': [target_name_id_celltype_dict[label] for label in labels],\n",
    "              'pred_cellcode': pred_cellcodes,\n",
    "              'gt_cellcode': labels,\n",
    "              'confidence_socre': confidence_socre\n",
    "              }\n",
    "    pd.DataFrame(data=d_save).to_csv(f'{output_dir}/Geneformer_dataset{dataset_name}_result.csv')\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    # save confidence score and label to csv\n",
    "    confidence_score = nn.Softmax(dim=-1)(torch.from_numpy(pred.predictions)).detach().cpu().numpy().tolist()\n",
    "    labels_tocsv = pred.label_ids.tolist()\n",
    "    SaveProcessResult(labels_tocsv, preds, confidence_score)\n",
    "\n",
    "    # calculate accuracy and macro f1 using sklearn's function\n",
    "    acc = balanced_accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='macro')\n",
    "    recall = recall_score(labels, preds, average='macro')\n",
    "    macro_f1 = f1_score(labels, preds, average='macro')\n",
    "\n",
    "    micro_precision = precision_score(labels, preds, average='micro')\n",
    "    micro_recall = recall_score(labels, preds, average='micro')\n",
    "    micro_f1 = f1_score(labels, preds, average='micro')\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'macro_f1': macro_f1,\n",
    "        \"micro_precision\": micro_precision,\n",
    "        \"micro_recall\": micro_recall,\n",
    "        \"micro_f1\": micro_f1\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "# set model parameters\n",
    "# max input size\n",
    "max_input_size = 2 ** 11  # 2048\n",
    "max_lr = args.lr\n",
    "# how many pretrained layers to freeze\n",
    "freeze_layers = 0\n",
    "# number gpus\n",
    "num_gpus = 1\n",
    "# number cpu cores\n",
    "num_proc = 16\n",
    "# batch size for training and eval\n",
    "geneformer_batch_size =args.batch_size\n",
    "# learning schedule\n",
    "lr_schedule_fn = \"linear\"\n",
    "# warmup steps\n",
    "warmup_steps = 500\n",
    "# number of epochs\n",
    "epochs = args.epoch\n",
    "# optimizer\n",
    "optimizer = \"adamw\"\n",
    "\n",
    "\n",
    "# %%\n",
    "celltype_id_labels = [target_name_id_dict[item] for item in train_dataset[\"cell_type\"]]\n",
    "class_num = np.unique(celltype_id_labels, return_counts=True)[1].tolist()\n",
    "class_weight = torch.tensor([(1 - (x / sum(class_num))) ** 2 for x in class_num])\n",
    "weighted_loss = nn.CrossEntropyLoss(weight=class_weight)\n",
    "\n",
    "organ_trainset = trainset_dict[0]\n",
    "organ_evalset = evalset_dict[0]\n",
    "organ_testset = testset_dict[0]\n",
    "organ_label_dict = traintargetdict_dict[0]\n",
    "logging_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36a9496ca9c9c282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/PCON0022/yuy702/Geneformer/encoder_prompt/ms/1\n",
      "-----------------------------------------------------------------------------------------\n",
      "Learnable params bert.encoder.layer.0.attention.Space_Adapter.D_fc1.weight with shape torch.Size([128, 512])\n",
      "Learnable params bert.encoder.layer.0.attention.Space_Adapter.D_fc1.bias with shape torch.Size([128])\n",
      "Learnable params bert.encoder.layer.0.attention.Space_Adapter.D_fc2.weight with shape torch.Size([512, 128])\n",
      "Learnable params bert.encoder.layer.0.attention.Space_Adapter.D_fc2.bias with shape torch.Size([512])\n",
      "Learnable params bert.encoder.layer.1.attention.Space_Adapter.D_fc1.weight with shape torch.Size([128, 512])\n",
      "Learnable params bert.encoder.layer.1.attention.Space_Adapter.D_fc1.bias with shape torch.Size([128])\n",
      "Learnable params bert.encoder.layer.1.attention.Space_Adapter.D_fc2.weight with shape torch.Size([512, 128])\n",
      "Learnable params bert.encoder.layer.1.attention.Space_Adapter.D_fc2.bias with shape torch.Size([512])\n",
      "Learnable params bert.encoder.layer.2.attention.Space_Adapter.D_fc1.weight with shape torch.Size([128, 512])\n",
      "Learnable params bert.encoder.layer.2.attention.Space_Adapter.D_fc1.bias with shape torch.Size([128])\n",
      "Learnable params bert.encoder.layer.2.attention.Space_Adapter.D_fc2.weight with shape torch.Size([512, 128])\n",
      "Learnable params bert.encoder.layer.2.attention.Space_Adapter.D_fc2.bias with shape torch.Size([512])\n",
      "Learnable params bert.encoder.layer.3.attention.Space_Adapter.D_fc1.weight with shape torch.Size([128, 512])\n",
      "Learnable params bert.encoder.layer.3.attention.Space_Adapter.D_fc1.bias with shape torch.Size([128])\n",
      "Learnable params bert.encoder.layer.3.attention.Space_Adapter.D_fc2.weight with shape torch.Size([512, 128])\n",
      "Learnable params bert.encoder.layer.3.attention.Space_Adapter.D_fc2.bias with shape torch.Size([512])\n",
      "Learnable params bert.encoder.layer.4.attention.Space_Adapter.D_fc1.weight with shape torch.Size([128, 512])\n",
      "Learnable params bert.encoder.layer.4.attention.Space_Adapter.D_fc1.bias with shape torch.Size([128])\n",
      "Learnable params bert.encoder.layer.4.attention.Space_Adapter.D_fc2.weight with shape torch.Size([512, 128])\n",
      "Learnable params bert.encoder.layer.4.attention.Space_Adapter.D_fc2.bias with shape torch.Size([512])\n",
      "Learnable params bert.encoder.layer.5.attention.Space_Adapter.D_fc1.weight with shape torch.Size([128, 512])\n",
      "Learnable params bert.encoder.layer.5.attention.Space_Adapter.D_fc1.bias with shape torch.Size([128])\n",
      "Learnable params bert.encoder.layer.5.attention.Space_Adapter.D_fc2.weight with shape torch.Size([512, 128])\n",
      "Learnable params bert.encoder.layer.5.attention.Space_Adapter.D_fc2.bias with shape torch.Size([512])\n",
      "Learnable params bert.encoder.layer.6.attention.Space_Adapter.D_fc1.weight with shape torch.Size([128, 512])\n",
      "Learnable params bert.encoder.layer.6.attention.Space_Adapter.D_fc1.bias with shape torch.Size([128])\n",
      "Learnable params bert.encoder.layer.6.attention.Space_Adapter.D_fc2.weight with shape torch.Size([512, 128])\n",
      "Learnable params bert.encoder.layer.6.attention.Space_Adapter.D_fc2.bias with shape torch.Size([512])\n",
      "Learnable params bert.encoder.layer.7.attention.Space_Adapter.D_fc1.weight with shape torch.Size([128, 512])\n",
      "Learnable params bert.encoder.layer.7.attention.Space_Adapter.D_fc1.bias with shape torch.Size([128])\n",
      "Learnable params bert.encoder.layer.7.attention.Space_Adapter.D_fc2.weight with shape torch.Size([512, 128])\n",
      "Learnable params bert.encoder.layer.7.attention.Space_Adapter.D_fc2.bias with shape torch.Size([512])\n",
      "Learnable params bert.encoder.layer.8.attention.Space_Adapter.D_fc1.weight with shape torch.Size([128, 512])\n",
      "Learnable params bert.encoder.layer.8.attention.Space_Adapter.D_fc1.bias with shape torch.Size([128])\n",
      "Learnable params bert.encoder.layer.8.attention.Space_Adapter.D_fc2.weight with shape torch.Size([512, 128])\n",
      "Learnable params bert.encoder.layer.8.attention.Space_Adapter.D_fc2.bias with shape torch.Size([512])\n",
      "Learnable params bert.encoder.layer.9.attention.Space_Adapter.D_fc1.weight with shape torch.Size([128, 512])\n",
      "Learnable params bert.encoder.layer.9.attention.Space_Adapter.D_fc1.bias with shape torch.Size([128])\n",
      "Learnable params bert.encoder.layer.9.attention.Space_Adapter.D_fc2.weight with shape torch.Size([512, 128])\n",
      "Learnable params bert.encoder.layer.9.attention.Space_Adapter.D_fc2.bias with shape torch.Size([512])\n",
      "Learnable params bert.encoder.layer.10.attention.Space_Adapter.D_fc1.weight with shape torch.Size([128, 512])\n",
      "Learnable params bert.encoder.layer.10.attention.Space_Adapter.D_fc1.bias with shape torch.Size([128])\n",
      "Learnable params bert.encoder.layer.10.attention.Space_Adapter.D_fc2.weight with shape torch.Size([512, 128])\n",
      "Learnable params bert.encoder.layer.10.attention.Space_Adapter.D_fc2.bias with shape torch.Size([512])\n",
      "Learnable params bert.encoder.layer.11.attention.Space_Adapter.D_fc1.weight with shape torch.Size([128, 512])\n",
      "Learnable params bert.encoder.layer.11.attention.Space_Adapter.D_fc1.bias with shape torch.Size([128])\n",
      "Learnable params bert.encoder.layer.11.attention.Space_Adapter.D_fc2.weight with shape torch.Size([512, 128])\n",
      "Learnable params bert.encoder.layer.11.attention.Space_Adapter.D_fc2.bias with shape torch.Size([512])\n",
      "Learnable params bert.pooler.dense.weight with shape torch.Size([512, 512])\n",
      "Learnable params bert.pooler.dense.bias with shape torch.Size([512])\n",
      "Learnable params classifier.weight with shape torch.Size([18, 512])\n",
      "Learnable params classifier.bias with shape torch.Size([18])\n",
      "Total Pre freeze Params: 41.15M\n",
      "Total Post freeze Params: 1.85M\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "        # \"../Pretrain_ckpts/geneformer-12L-30M-prompt\",\n",
    "        model_path+str(i),\n",
    "        num_labels=len(organ_label_dict.keys()),\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        ignore_mismatched_sizes=True).to(\"cuda\")\n",
    "prompt_type = model.config.prompt_type\n",
    "output_dir = args.output_path + prompt_type + '/' + dataset_name + f\"/{i}\"\n",
    "model.config.save_pretrained(output_dir)\n",
    "print(output_dir)\n",
    "pre_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters()).values())\n",
    "\n",
    "keywords = [\n",
    "    'lora_A',\n",
    "    'lora_B',\n",
    "    'lora_value.bias',\n",
    "    'lora_key.bias',\n",
    "    'lora_query.bias',\n",
    "    'prompt_embeddings',\n",
    "    'adapter',\n",
    "    'Adapter'\n",
    "]\n",
    "\n",
    "for name, para in model.named_parameters():\n",
    "    para.requires_grad = False\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.bert.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "for name, para in model.named_parameters():\n",
    "    if any(keyword in name for keyword in keywords):\n",
    "        para.requires_grad = True\n",
    "\n",
    "print(\"-\" * 89)\n",
    "learnable_params = {k: v for k, v in model.named_parameters() if v.requires_grad == True}\n",
    "for k, v in learnable_params.items():\n",
    "    print(f\"Learnable params {k} with shape {v.shape}\")\n",
    "\n",
    "post_freeze_param_count = sum(\n",
    "    dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "print(\"Total Pre freeze Params: %.2fM\" % (pre_freeze_param_count / 1e6,))\n",
    "print(\"Total Post freeze Params: %.2fM\" % (post_freeze_param_count / 1e6,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/users/PCON0022/yuy702/Geneformer/encoder_prompt/ms/1’: File exists\n",
      "/fs/ess/PAS1475/Yang/yy/env/Geneformer/lib/python3.8/site-packages/accelerate/accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/users/PCON0022/yuy702/Geneformer/example_py/../geneformer/collator_for_classification.py:582: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PCON0022/yuy702/Geneformer/example_py/../geneformer/collator_for_classification.py:582: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold1:macro Accuracy: 0.842, macro Precision: 0.790,macro Recall: 0.842, macro F1: 0.800\n",
      "fold1:micro Accuracy: 0.842, micro Precision: 0.829, micro Recall: 0.829, micro F1: 0.829\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MyTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        if model.config.prompt_type == \"prefix_prompt\":\n",
    "            mask_cat = torch.ones([inputs[\"input_ids\"].shape[0], model.config.num_token]).to(\"cuda\")\n",
    "            mask_middle = torch.cat((inputs[\"attention_mask\"][:, :1], mask_cat), dim=1)\n",
    "            inputs[\"attention_mask\"] = torch.cat((mask_middle, inputs[\"attention_mask\"][:, 1:]), dim=1)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        loss_fct = weighted_loss.to(outputs['logits'].device)\n",
    "        loss = loss_fct(outputs['logits'], inputs['labels'])\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Returns the training [`~torch.utils.data.DataLoader`].\n",
    "\n",
    "        Will use no sampler if `train_dataset` does not implement `__len__`, a random sampler (adapted to distributed\n",
    "        training if necessary) otherwise.\n",
    "\n",
    "        Subclass and override this method if you want to inject some custom behavior.\n",
    "        \"\"\"\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "\n",
    "        train_dataset = self.train_dataset\n",
    "        data_collator = self.data_collator\n",
    "        if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n",
    "            train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n",
    "        else:\n",
    "            data_collator = self._get_collator_with_removed_columns(data_collator, description=\"training\")\n",
    "\n",
    "        dataloader_params = {\n",
    "            \"batch_size\": self._train_batch_size,\n",
    "            \"collate_fn\": data_collator,\n",
    "            \"num_workers\": self.args.dataloader_num_workers,\n",
    "            \"pin_memory\": self.args.dataloader_pin_memory,\n",
    "        }\n",
    "\n",
    "        if not isinstance(train_dataset, torch.utils.data.IterableDataset):\n",
    "            train_class_num = np.unique(train_dataset[\"label\"], return_counts=True)[1]\n",
    "            sample_weights = 1.0 / train_class_num[train_dataset[\"label\"]]\n",
    "            sample_weights = sample_weights / np.sum(sample_weights)\n",
    "            train_num = train_dataset.shape[0]\n",
    "            train_sampler = torch.utils.data.sampler.WeightedRandomSampler(sample_weights, train_num,\n",
    "                                                                           replacement=True)\n",
    "            dataloader_params[\"sampler\"] = train_sampler\n",
    "            dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
    "            dataloader_params[\"worker_init_fn\"] = seed_worker\n",
    "\n",
    "        return self.accelerator.prepare(DataLoader(train_dataset, **dataloader_params))\n",
    "\n",
    "current_date = datetime.datetime.now()\n",
    "datestamp = f\"{str(current_date.year)[-2:]}-{current_date.month:02d}-{current_date.day:02d}_{current_date.hour:02d}:{current_date.minute:02d}:{current_date.second:02d}\"\n",
    "\n",
    "# ensure not overwriting previously saved model\n",
    "saved_model_test = os.path.join(output_dir, f\"pytorch_model.bin\")\n",
    "model.config.save_pretrained(output_dir)\n",
    "# print(datestamp)\n",
    "if os.path.isfile(saved_model_test) == True:\n",
    "    raise Exception(\"Model already saved to this directory.\")\n",
    "\n",
    "# make output directory\n",
    "subprocess.call(f'mkdir {output_dir}', shell=True)\n",
    "# set training arguments\n",
    "training_args = {\n",
    "    \"learning_rate\": max_lr,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"logging_steps\": logging_steps,\n",
    "    \"group_by_length\": True,\n",
    "    \"length_column_name\": \"length\",\n",
    "    \"disable_tqdm\": False,\n",
    "    \"lr_scheduler_type\": lr_schedule_fn,\n",
    "    \"warmup_steps\": warmup_steps,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"per_device_train_batch_size\": geneformer_batch_size,\n",
    "    \"per_device_eval_batch_size\": geneformer_batch_size,\n",
    "    \"num_train_epochs\": epochs,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"output_dir\": output_dir,\n",
    "    \"save_safetensors\": False,\n",
    "}\n",
    "\n",
    "training_args_init = TrainingArguments(**training_args)\n",
    "\n",
    "# create the trainer\n",
    "trainer = MyTrainer(\n",
    "    model=model,\n",
    "    args=training_args_init,\n",
    "    data_collator=DataCollatorForCellClassification(),\n",
    "    train_dataset=organ_trainset,\n",
    "    eval_dataset=organ_evalset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")\n",
    "# train the cell type classifier\n",
    "# trainer.train()\n",
    "predictions = trainer.predict(organ_testset)\n",
    "\n",
    "print(f\"fold{i}:\"\n",
    "      f\"macro Accuracy: {predictions.metrics['test_accuracy']:.3f}, macro Precision: {predictions.metrics['test_precision']:.3f},macro Recall: {predictions.metrics['test_recall']:.3f}, \"\n",
    "      f\"macro F1: {predictions.metrics['test_macro_f1']:.3f}\"\n",
    "      )\n",
    "print(f\"fold{i}:\"\n",
    "      f\"micro Accuracy: {predictions.metrics['test_accuracy']:.3f}, micro Precision: {predictions.metrics['test_micro_precision']:.3f}, micro Recall: {predictions.metrics['test_micro_recall']:.3f}, \"\n",
    "      f\"micro F1: {predictions.metrics['test_micro_f1']:.3f}\")\n",
    "\n",
    "with open(f\"{output_dir}/predictions.pickle\", \"wb\") as fp:\n",
    "    pickle.dump(predictions, fp)\n",
    "trainer.save_metrics(\"eval\", predictions.metrics)\n",
    "trainer.save_model(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dee8db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
